{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and download NLTK resources\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from os.path import join\n",
    "from nltk import pos_tag\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import randint as sp_randint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "'''nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to data directory\n",
    "data_dir = join(os.getcwd(), \"Data\")\n",
    "\n",
    "# Define data sets\n",
    "training_set = 'twitter-training-data.txt'\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'negative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'negative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "    return '%.3f' % semevalmacrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read in training set, dev set, and testing sets\n",
    "def read_in_file(data_dir, filename):\n",
    "    data = []\n",
    "    with io.open(join(data_dir, filename), 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "    return data\n",
    "        \n",
    "def preprocessing(data):\n",
    "    #remove the first two sections of each line in data, keeping just the last column\n",
    "    tweet_content = [line.split(\"\\t\")[-1] for line in data]\n",
    "\n",
    "    #use regex to remove all urls\n",
    "    def remove_urls(text):\n",
    "        url_regex = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        return url_regex.sub('', text)\n",
    "\n",
    "    #use regex to remove all words that start with @\n",
    "    def remove_at(text):\n",
    "        at_regex = re.compile(r'@[a-zA-Z0-9_]+')\n",
    "        return at_regex.sub('', text)\n",
    "\n",
    "    #use regex to remove all special characters\n",
    "    def remove_special_characters(text):\n",
    "        special_characters_regex = re.compile(r'[^a-zA-Z0-9 ]')\n",
    "        return special_characters_regex.sub('', text)\n",
    "\n",
    "    #use regex to remove numbers that are not part of a word\n",
    "    def remove_numbers(text):\n",
    "        numbers_regex = re.compile(r'(?<![a-zA-Z0-9_])\\d+(?![a-zA-Z0-9_])')\n",
    "        return numbers_regex.sub('', text)\n",
    "    \n",
    "    #use regex to remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        return' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "    #run the above functions on each line of data\n",
    "    tweet_content = [remove_stopwords(remove_numbers(remove_special_characters(remove_at(remove_urls(line))))) for line in tweet_content]\n",
    "\n",
    "    #replace the last item in each line of data with the same line in tweet_content\n",
    "    for i, line in enumerate(data):\n",
    "        data[i] = line.replace(line.split(\"\\t\")[-1], tweet_content[i]) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    data[dataset] = [preprocessing(read_in_file(data_dir, dataset))]\n",
    "    tweets[dataset] = [line.split(\"\\t\")[2] for line in preprocessing(read_in_file(data_dir, dataset))]\n",
    "    tweetids[dataset] = [line.split(\"\\t\")[0] for line in preprocessing(read_in_file(data_dir, dataset))]\n",
    "    tweetgts[dataset] = [line.split(\"\\t\")[1] for line in preprocessing(read_in_file(data_dir, dataset))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize each line of tweets\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    try:\n",
    "        tweets[dataset] = [word_tokenize(line) for line in tweets[dataset]]\n",
    "    except:\n",
    "        tweets[dataset] = [word_tokenize(line) for line in [line.split(\"\\t\")[2] for line in preprocessing(read_in_file(data_dir, dataset))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the part of speech tagging in wordnet format for words in tweets\n",
    "def pos_tagging(word):\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith(\"J\"):\n",
    "        return \"a\"\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return \"v\"\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return \"n\"\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "#lemmatize the words in tweets\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    for i, tweet in enumerate(tweets[dataset]):\n",
    "        for j, word in enumerate(tweet):\n",
    "            if pos_tagging(word) != \"\":\n",
    "                tweets[dataset][i][j] = lemmatizer.lemmatize(word, pos=pos_tagging(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove the file extension from a .txt file\n",
    "def remove_extension(filename):\n",
    "    return filename.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt, privileged, play, foo, fighter, song, guitar, today, one, plectrum, gig, saturday\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(tweets[training_set][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting training set and test sets\n",
    "training_set = 'twitter-training-data.txt'\n",
    "testset = 'twitter-test1.txt'\n",
    "testset_path = data_dir + '\\\\' + testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature generation function\n",
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, tweets, tweetgts, word_vectors, max_tweet_length):\n",
    "        self.tweets = tweets\n",
    "        self.labels = tweetgts\n",
    "        self.word_vectors = word_vectors\n",
    "        self.max_tweet_length = max_tweet_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets[idx].split(',')\n",
    "        tweet_vector = []\n",
    "        for word in tweet:\n",
    "            word = ''.join(filter(str.isalnum, word))\n",
    "            if word in self.word_vectors:\n",
    "                tweet_vector.append(self.word_vectors[word])\n",
    "        tweet_vector = np.asarray(tweet_vector)\n",
    "        if len(tweet_vector) > 0:\n",
    "            tweet_vector = np.pad(tweet_vector, \n",
    "                    ((0, self.max_tweet_length - len(tweet_vector)), (0,0)), \n",
    "                    'constant', constant_values=0)\n",
    "        else: \n",
    "            tweet_vector = np.zeros((self.max_tweet_length, 100))\n",
    "        label = self.labels[idx]\n",
    "        if label == 'positive':\n",
    "            label = 0.0\n",
    "        elif label == 'negative':\n",
    "            label = 1.0\n",
    "        else:\n",
    "            label = 2.0\n",
    "        return tweet_vector.astype(np.float32), label\n",
    "    \n",
    "def generate_features(training_set, testset, feature: str):\n",
    "    training_tweets = [', '.join(tweet) for tweet in tweets[training_set]]\n",
    "    test_tweets = [', '.join(tweet) for tweet in tweets[testset]]\n",
    "    if feature == 'bow':\n",
    "        #create feature vector using tfidf for bag of words\n",
    "        vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "        train_vectors = vectorizer.fit_transform(training_tweets)\n",
    "        test_vectors = vectorizer.transform(test_tweets)\n",
    "\n",
    "    elif feature == 'embeddings':\n",
    "        #word embedding using glove\n",
    "        glove_path = data_dir + '\\\\glove.6B\\\\' + 'glove.6B.100d.txt'\n",
    "        word_vectors = {}\n",
    "\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.split()[0]\n",
    "                vector = np.asarray(line.split()[1:])\n",
    "                word_vectors[word] = vector\n",
    "                \n",
    "        #finding the maximum tweet length for padding\n",
    "        longest_training_length = 0\n",
    "        for tweet in training_tweets:\n",
    "            tweet = tweet.split(',')\n",
    "            length = len(tweet)\n",
    "            if length > longest_training_length:\n",
    "                longest_training_length = length\n",
    "        \n",
    "        longest_test_length = 0\n",
    "        for tweet in test_tweets:\n",
    "            tweet = tweet.split(',')\n",
    "            length = len(tweet)\n",
    "            if length > longest_test_length:\n",
    "                longest_test_length = length\n",
    "        \n",
    "        batch_size = 128\n",
    "        train_dataset = TweetsDataset(training_tweets, tweetgts[training_set], word_vectors, longest_training_length)\n",
    "        train_vectors = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_dataset = TweetsDataset(test_tweets, tweetgts[testset], word_vectors, longest_test_length)\n",
    "        test_vectors = DataLoader(test_dataset, batch_size=batch_size)\n",
    "            \n",
    "    return train_vectors, test_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform svm classification with a linear kernel\n",
    "train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "svm = LinearSVC()\n",
    "svm.fit(train_vectors, tweetgts[training_set])\n",
    "predictions = svm.predict(test_vectors)\n",
    "\n",
    "#testing on the testsets\n",
    "idpreds = {}\n",
    "testset_path = data_dir + '\\\\' + testset\n",
    "for i, key in enumerate(tweetids[testset]):\n",
    "    idpreds[key] = predictions[i]\n",
    "\n",
    "evaluate(idpreds, testset_path, 'svm')\n",
    "#confusion(idpreds, testset_path, 'svm')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells are for the randomforest classifier. Two methods, grid search and random search were used to find the optimal parameters for the classifier but take too long to run for it to be feasible to include in the actual classifier. The optimal number of estimators found, 138, was surprisingly consistent for all the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest classifier for the twitter training data\n",
    "train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "ids = []\n",
    "for i, key in enumerate(tweetids[training_set]):\n",
    "    ids.append(key)\n",
    "randomforest = RandomForestClassifier(n_estimators = 138, max_depth = 50, random_state = 0)\n",
    "randomforest.fit(train_vectors, tweetgts[training_set])\n",
    "predictions = randomforest.predict(test_vectors)\n",
    "\n",
    "#testing on the testsets\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "testset_path = data_dir + '\\\\' + testset\n",
    "\n",
    "evaluate(idpreds, testset_path, 'randomforest')\n",
    "confusion(idpreds, testset_path, 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting to optimise the randomforest classifier\n",
    "#this will take like a year to run \n",
    "\n",
    "#data preprocessing\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_vectors = scaler.fit_transform(train_vectors)\n",
    "test_vectors = scaler.transform(test_vectors)\n",
    "\n",
    "#feature selection\n",
    "selector = SelectKBest(f_classif, k=400)\n",
    "train_vectors = selector.fit_transform(train_vectors, tweetgts[training_set])\n",
    "test_vectors = selector.transform(test_vectors)\n",
    "\n",
    "#hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 40],\n",
    "    'max_depth': [25, 35, 45],\n",
    "    'max_features':[1,3,5,7],\n",
    "    'min_samples_leaf':[1,2,3],\n",
    "    'min_samples_split':[1,2,3]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid=param_grid, cv=5, verbose=1)\n",
    "grid.fit(train_vectors, tweetgts[training_set])\n",
    "\n",
    "#testing on the testsets\n",
    "randomforest = grid.best_estimator_\n",
    "predictions = randomforest.predict(test_vectors)\n",
    "\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "testset_path = data_dir + '\\\\' + testset\n",
    "print(randomforest)\n",
    "evaluate(idpreds, testset_path, 'randomforest')\n",
    "confusion(idpreds, testset_path, 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting to optimise the randomforest classifier using the random search instead of grid search\n",
    "#this will take like a year to run \n",
    "\n",
    "#data preprocessing\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_vectors = scaler.fit_transform(train_vectors)\n",
    "test_vectors = scaler.transform(test_vectors)\n",
    "\n",
    "#feature selection\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "train_vectors = selector.fit_transform(train_vectors, tweetgts[training_set])\n",
    "test_vectors = selector.transform(test_vectors)\n",
    "\n",
    "#hyperparameter tuning\n",
    "param_dist = {\"n_estimators\": sp_randint(50, 500),\n",
    "              \"max_depth\": [50, 60, 70, None]}\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=0), \n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=5, random_state=0, verbose=1)\n",
    "random_search.fit(train_vectors, tweetgts[training_set])\n",
    "\n",
    "#testing on the testsets\n",
    "randomforest = random_search.best_estimator_\n",
    "predictions = randomforest.predict(test_vectors)\n",
    "\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "print(randomforest)\n",
    "evaluate(idpreds, testset_path, 'randomforest')\n",
    "confusion(idpreds, testset_path, 'randomforest')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells contain the knn classifier. The second is not used in the main code but was used to find the optimal hyperparameters. It's not included as the actual classifier because running it across enough parameters to be useful takes so long, however it is left here to show the method use to find the parameters in the actual classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn classifier for the twitter training data\n",
    "train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "knn = KNeighborsClassifier(n_neighbors = 100, weights = 'uniform', algorithm = 'auto', leaf_size = 300, p = 2)\n",
    "knn.fit(train_vectors, tweetgts[training_set]) \n",
    "predictions = knn.predict(test_vectors)\n",
    "            \n",
    "#testing on the testsets\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "\n",
    "print(predictions)\n",
    "evaluate(idpreds, testset_path, 'knn')\n",
    "confusion(idpreds, testset_path, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting to optimise the knn classifier\n",
    "train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "#data preprocessing\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_vectors = scaler.fit_transform(train_vectors)\n",
    "test_vectors = scaler.transform(test_vectors)\n",
    "\n",
    "#feature selection\n",
    "selector = SelectKBest(chi2, k='all')\n",
    "train_vectors = selector.fit_transform(train_vectors, tweetgts[training_set])\n",
    "test_vectors = selector.transform(test_vectors)\n",
    "\n",
    "#parameter tuning\n",
    "k_values = [90, 110, 130, 150, 170]\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "best_accuracy = 0\n",
    "best_k = None\n",
    "best_metric = None\n",
    "for k in k_values:\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='auto', leaf_size=300, p=2, metric=metric)\n",
    "        knn.fit(train_vectors, tweetgts[training_set]) \n",
    "        predictions = knn.predict(test_vectors)\n",
    "        accuracy = accuracy_score(tweetgts[testset], predictions)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "            best_metric = metric\n",
    "\n",
    "#testing on the testsets\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights='uniform', algorithm='auto', leaf_size=300, p=2, metric=best_metric)\n",
    "knn.fit(train_vectors, tweetgts[training_set]) \n",
    "predictions = knn.predict(test_vectors)\n",
    "\n",
    "print(best_accuracy, best_k, best_metric)\n",
    "\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "evaluate(idpreds, testset_path, 'knn')\n",
    "confusion(idpreds, testset_path, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the lstm\n",
    "#train_vectors, test_vectors = generate_features(training_set, testset, 'embeddings')\n",
    "\n",
    "#creating the input vector\n",
    "for i, batch in enumerate(train_vectors):\n",
    "    input, target_vector = batch\n",
    "    \n",
    "    #defining the models\n",
    "    batch_size = target_vector.shape[0]\n",
    "    hidden_size = 64\n",
    "    bidirectional = True\n",
    "    lstm = torch.nn.LSTM(input_size=input.shape[-1], hidden_size=hidden_size, num_layers=2, batch_first=True, bidirectional=bidirectional)\n",
    "    output_layer = torch.nn.Linear(hidden_size+hidden_size*bidirectional, 1)\n",
    "    \n",
    "    #adding dropout\n",
    "    dropout_rate = 0.21\n",
    "    dropout = torch.nn.Dropout(0.21)\n",
    "\n",
    "    #defining the loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    #defining the optimizer\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)\n",
    "\n",
    "    #training the model\n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "        target = torch.tensor(target_vector).long()\n",
    "        target = target.reshape(target.shape[0], 1)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = lstm(input)\n",
    "        output = dropout(output)\n",
    "        output = output_layer(output)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#testing the model\n",
    "all_test_outputs = []\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_vectors):\n",
    "        test_input = batch[0]\n",
    "        test_output, _ = lstm(test_input)\n",
    "        test_output = dropout(test_output)\n",
    "        test_output = output_layer(test_output)\n",
    "        all_test_outputs.append(test_output)\n",
    "\n",
    "#concatenate test output tensors from all batches into a single tensor\n",
    "all_test_outputs = torch.cat(all_test_outputs, dim=0)\n",
    "\n",
    "#make predictions using the concatenated test outputs\n",
    "predictions = all_test_outputs.argmax(dim=1)\n",
    "\n",
    "#converting to string output\n",
    "predictions_list = []\n",
    "for i, index in enumerate(predictions):\n",
    "    if index == 0:\n",
    "        predictions_list.append('positive')\n",
    "    elif index == 1:\n",
    "        predictions_list.append('negative')\n",
    "    elif index == 2:\n",
    "        predictions_list.append('neutral')\n",
    "\n",
    "#evaluation\n",
    "testset_path = data_dir + '\\\\' + testset\n",
    "idpreds = {key: pred for key, pred in zip(tweetids[testset], predictions_list)}\n",
    "evaluate(idpreds, testset_path, 'lstm')\n",
    "confusion(idpreds, testset_path, 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svm\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test1.txt (bow-svm): 0.552\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test2.txt (bow-svm): 0.578\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test3.txt (bow-svm): 0.535\n",
      "Training randomforest\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test1.txt (bow-randomforest): 0.360\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test2.txt (bow-randomforest): 0.393\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test3.txt (bow-randomforest): 0.336\n",
      "Training knn\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test1.txt (bow-knn): 0.319\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test2.txt (bow-knn): 0.351\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test3.txt (bow-knn): 0.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benla\\AppData\\Local\\Temp\\ipykernel_14676\\4159776461.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target_vector).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test1.txt (embeddings-LSTM): 0.318\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test2.txt (embeddings-LSTM): 0.311\n",
      "d:\\Documents\\CS_MSc\\2-Natural-Language-Processing\\Coursework\\Data\\twitter-test3.txt (embeddings-LSTM): 0.330\n"
     ]
    }
   ],
   "source": [
    "# Build traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "classifier_names = ['svm', 'randomforest', 'knn', 'LSTM']\n",
    "feature_names = ['bow', 'embeddings']\n",
    "\n",
    "for classifier in classifier_names:\n",
    "    for features in feature_names:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if features == 'embeddings' and classifier != 'LSTM':\n",
    "            continue\n",
    "        \n",
    "        if classifier == 'svm':\n",
    "            # write the svm classifier here\n",
    "            #svm classifier\n",
    "            train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "            svm = LinearSVC()\n",
    "            svm.fit(train_vectors, tweetgts[training_set])\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'randomforest':\n",
    "            # write the classifier 2 here\n",
    "            #randomforest classifier\n",
    "            train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "            ids = []\n",
    "            for i, key in enumerate(tweetids[training_set]):\n",
    "                ids.append(key)\n",
    "            randomforest = RandomForestClassifier(n_estimators = 138, max_depth = 50, random_state = 0)\n",
    "            randomforest.fit(train_vectors, tweetgts[training_set])\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'knn':\n",
    "            # write the classifier 3 here\n",
    "            #knn classifier\n",
    "            train_vectors = generate_features(training_set, testset, features)[0]\n",
    "            knn = KNeighborsClassifier(n_neighbors = 110, weights = 'uniform', algorithm = 'auto', leaf_size = 300, p = 2)\n",
    "            knn.fit(train_vectors, tweetgts[training_set]) \n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "        \n",
    "            train_vectors, test_vectors = generate_features(training_set, testset, features)\n",
    "            #creating the input vector\n",
    "            for i, batch in enumerate(train_vectors):\n",
    "                input, target_vector = batch\n",
    "                \n",
    "                #defining the models\n",
    "                batch_size = target_vector.shape[0]\n",
    "                hidden_size = 64\n",
    "                bidirectional = True\n",
    "                lstm = torch.nn.LSTM(input_size=input.shape[-1], hidden_size=hidden_size, num_layers=2, batch_first=True, bidirectional=bidirectional)\n",
    "                output_layer = torch.nn.Linear(hidden_size+hidden_size*bidirectional, 1)\n",
    "                \n",
    "                #adding dropout\n",
    "                dropout_rate = 0.21\n",
    "                dropout = torch.nn.Dropout(0.21)\n",
    "\n",
    "                #defining the loss function\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                #defining the optimizer\n",
    "                optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)\n",
    "\n",
    "                #training the model\n",
    "                epochs = 15\n",
    "                for epoch in range(epochs):\n",
    "                    target = torch.tensor(target_vector).long()\n",
    "                    target = target.reshape(target.shape[0], 1)\n",
    "                    optimizer.zero_grad()\n",
    "                    output, _ = lstm(input)\n",
    "                    output = dropout(output)\n",
    "                    output = output_layer(output)\n",
    "                    loss = loss_fn(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Prediction performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            test_vectors = generate_features(training_set, testset, features)[1]\n",
    "            if classifier == 'svm':\n",
    "                predictions = svm.predict(test_vectors)\n",
    "            elif classifier == 'randomforest':\n",
    "                predictions = randomforest.predict(test_vectors)\n",
    "            elif classifier == 'knn':\n",
    "                predictions = knn.predict(test_vectors)\n",
    "            elif classifier == 'LSTM':\n",
    "                all_test_outputs = []\n",
    "                lstm.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(test_vectors):\n",
    "                        test_input = batch[0]\n",
    "                        test_output, _ = lstm(test_input)\n",
    "                        test_output = dropout(test_output)\n",
    "                        test_output = output_layer(test_output)\n",
    "                        all_test_outputs.append(test_output)\n",
    "\n",
    "                #concatenate test output tensors from all batches into a single tensor\n",
    "                all_test_outputs = torch.cat(all_test_outputs, dim=0)\n",
    "\n",
    "                #make predictions \n",
    "                prediction_tensor = all_test_outputs.argmax(dim=1)\n",
    "\n",
    "                #converting to string output\n",
    "                predictions = []\n",
    "                for i, index in enumerate(prediction_tensor):\n",
    "                    if index == 0:\n",
    "                        predictions.append('positive')\n",
    "                    elif index == 1:\n",
    "                        predictions.append('negative')\n",
    "                    elif index == 2:\n",
    "                        predictions.append('neutral')\n",
    "            id_preds = {key: pred for key, pred in zip(tweetids[testset], predictions)}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join(data_dir, testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7d25d78634a3d357a1c15b95f21efed47a04efbedf4e3eaa0afd689bd632c88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
